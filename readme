--- #Assignment 2 ---
# Student: Roger Bassons Renart & Andrew Choliy
# NetID: rb1018 & ayc41
# Tested on grep.cs.rutgers.edu

#Issues with our code

Our code, with virtual memory enabled, does work with Benchmark vectorMultiply but
doesn't work with externalCal and parallelCal benchmarks.
Our current design can only hold a memory as big as the physical region reserved
to threads (per thread).


#Overview

In order to implement our memory managment mechanism we use a Linked List to keep
track of both pages and dynamic allocations. All the bookkeeping is done inside
the 8MB char array.

The first 1MB is space reserved to pages bookkeeping and the next ~1MB is reserved for
OS allocations. This OS reserved space is a Linked List of dynamic size elements.

After the OS reserved space and until the last 4 pages we have the memory region
used for the threads. This region is initialized with pages (fixed size
elements) with the help of a Linked List (this Linked List is the one we keep at
the beginning of the memory). Then inside each page we have another Linked List
that can be used by the page's thread to allocate dynamic size elements.

The last 4 pages are another Linked List that is used by all the threads as a
shared region.

#Pages

When a thread tries to allocate more memory than a page can hold, the OS will
allocate extra pages as needed and put them contiguous. At our first
implementation we tried to keep the Linked List between pages but we changed our
design to the current one(bookkeeping at the beginning). We changed it because it gets
difficult to manage multiple pages per process when there is Linked List nodes
in between. 

# Virtual Memory
To simulate virtual memory we swap pages to the beginning of the memory
(beginning of threads region).

This design makes impossible to give a thread a virtual memory bigger than the
physical memory. Because we allocate to contiguous pages, once we filled up
until the last page we can't allocate one more page to the thread. To do so we
should implement a better virtual memory with a page table and address translation.


#Swap & Replacing


#Tests

* test ... test3: Same as assignment 1
* test4: General test for virtual memory
* test5: Per thread's allocation test
* test6: Allocation bigger than a page test
* test7: Shared memory test



--- #Assignment 1 ---

# Student: Roger Bassons Renart
# NetID: rb1018
# Tested on grep.cs.rutgers.edu

## Implementation overview

---

* I implemented a RR scheduler that priority to jobs that finish before the fixed 
time quanta (25ms). Around every 4 seconds(~150 scheduler runs). I reorder the 
run queue giving priority to the oldest jobs no matter what. This could be improved 
(with more time) to avoid that if there are a lot of low priority jobs they 
don't starve to run. The way the scheduler works causes priority inversion: oldest
threads (low priority) get pushed to the front of the running queue every 4
seconds to aviod starvation.

* My implementation of the waiting queue is not global: there is a waiting queue for
each mutex. This way, a thread that is waiting for a mutex is never scheduled to run.
Once the mutex is released, the next thread waiting for the mutex is pushed to the
run queue. If no threads are waiting for the mutex, the mutex is completely unlocked.
This mutex implementation doesn't induce priority inversion because waiting threads
aren't scheduled to run, therefore waiting threads' priority is never lowered.

* I developed a Linked List to implement thread queues.

* In order to aviod being interrupted while running the scheduler I use sigprocmask 
function to block and ignore the interrupt signal.

* Scheduler variables, i.e. run queue, are allocated on the heap(malloc) instead of the 
stack to avoid losing changes when switching context.

* In addition to the pthreads implementation you will find three test programs 
that check the correctness of the implementation:

  - test1.c: checks pthread_create, pthread_exit, pthread_join among others.

  - test2.c: checks scheduler and pthread_yield among others (never ends).

  - test3.c: checks mutex among others.


## Benchmarking

---

I couldn't use the latest benchmarking code from the TA because unfortunately the
latest code was released too close to the assignment deadline and I've got a very tight
schedule. I modified the first versions of the benchmarking code to be able to
run it:

### vectorMultiply

threads 1000  
array size 100000000

my_pthread_t time: 31.901s

pthread_t    time: 25.489s


### parallelCal

threads 1000
C size 10000000
R size 100000

my_pthread_t
real 1m07.584s

pthread_t
real 26.239s
